{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f8e3b6-c555-48b9-96c3-75b77ca1a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/dccstor/hoo-misha-1/wilds/wilds/examples')\n",
    "sys.path.append('/dccstor/hoo-misha-1/wilds/WOODS')\n",
    "sys.path.append('/dccstor/hoo-misha-1/wilds/wilds')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "from models.bert.distilbert import DistilBertClassifier, DistilBertFeaturizer\n",
    "from configs.datasets import dataset_defaults\n",
    "\n",
    "import wilds\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.datasets.unlabeled.wilds_unlabeled_dataset import WILDSPseudolabeledSubset\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool, get_model_prefix, move_to\n",
    "from train import train, evaluate, infer_predictions,run_epoch\n",
    "from algorithms.initializer import initialize_algorithm, infer_d_out\n",
    "from transforms import initialize_transform\n",
    "\n",
    "from models.initializer import initialize_model\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported\n",
    "\n",
    "import torch.multiprocessing\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from examples.transforms import initialize_bert_transform\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import copy\n",
    "import re\n",
    "import psutil\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8253858-80a5-429d-97c2-a82ddb3dfc93",
   "metadata": {},
   "source": [
    "# Initialize Wilds Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4d13b4-e08c-444e-8e57-687ee303c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--breeds'], dest='breeds', nargs='?', const=True, default=False, type=<function parse_bool at 0x152f4b6be9d0>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Arg defaults are filled in according to examples/configs/ '''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required arguments\n",
    "parser.add_argument('-d', '--dataset', choices=wilds.supported_datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for dataset initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                    help='If true, tries to download the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes. Note that this also scales the test set down, so the reported numbers are not comparable with the full test set.')\n",
    "parser.add_argument('--version', default=None, type=str, help='WILDS labeled dataset version number.')\n",
    "\n",
    "# Unlabeled Dataset\n",
    "parser.add_argument('--unlabeled_split', default=None, type=str, choices=wilds.unlabeled_splits,  help='Unlabeled split to use. Some datasets only have some splits available.')\n",
    "parser.add_argument('--unlabeled_version', default=None, type=str, help='WILDS unlabeled dataset version number.')\n",
    "parser.add_argument('--use_unlabeled_y', default=False, type=parse_bool, const=True, nargs='?', \n",
    "                    help='If true, unlabeled loaders will also the true labels for the unlabeled data. This is only available for some datasets. Used for \"fully-labeled ERM experiments\" in the paper. Correct functionality relies on CrossEntropyLoss using ignore_index=-100.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--unlabeled_loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?', help='If true, sample examples such that batches are uniform over groups.')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?', help='If true, enforce groups sampled per batch are distinct.')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--unlabeled_n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--unlabeled_batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of batches to process before stepping optimizer and schedulers. If > 1, we simulate having a larger effective batch size (though batchnorm behaves differently).')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--noisystudent_add_dropout', type=parse_bool, const=True, nargs='?', help='If true, adds a dropout layer to the student model of NoisyStudent.')\n",
    "parser.add_argument('--noisystudent_dropout_rate', type=float)\n",
    "parser.add_argument('--pretrained_model_path', default=None, type=str, help='Specify a path to pretrained model weights')\n",
    "parser.add_argument('--load_featurizer_only', default=False, type=parse_bool, const=True, nargs='?', help='If true, only loads the featurizer weights and not the classifier weights.')\n",
    "\n",
    "# NoisyStudent-specific loading\n",
    "parser.add_argument('--teacher_model_path', type=str, help='Path to NoisyStudent teacher model weights. If this is defined, pseudolabels will first be computed for unlabeled data before anything else runs.')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--transform', choices=supported.transforms)\n",
    "parser.add_argument('--additional_train_transform', choices=supported.additional_transforms, help='Optional data augmentations to layer on top of the default transforms.')\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='The input resolution that images will be resized to before being passed into the model. For example, use --target_resolution 224 224 for a standard ResNet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "parser.add_argument('--randaugment_n', type=int, help='Number of RandAugment transformations to apply.')\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices=supported.losses)\n",
    "parser.add_argument('--loss_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for loss initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--wasserstein_blur', type=float, default=0.0001)\n",
    "parser.add_argument('--dann_penalty_weight', type=float)\n",
    "parser.add_argument('--dann_classifier_lr', type=float)\n",
    "parser.add_argument('--dann_featurizer_lr', type=float)\n",
    "parser.add_argument('--dann_discriminator_lr', type=float)\n",
    "parser.add_argument('--afn_penalty_weight', type=float)\n",
    "parser.add_argument('--safn_delta_r', type=float)\n",
    "parser.add_argument('--hafn_r', type=float)\n",
    "parser.add_argument('--use_hafn', default=False, type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--self_training_lambda', type=float)\n",
    "parser.add_argument('--self_training_threshold', type=float)\n",
    "parser.add_argument('--pseudolabel_T2', type=float, help='Percentage of total iterations at which to end linear scheduling and hold lambda at the max value')\n",
    "parser.add_argument('--soft_pseudolabels', default=False, type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--algo_log_metric')\n",
    "parser.add_argument('--process_pseudolabels_function', choices=supported.process_pseudolabels_functions)\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for optimizer initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for scheduler initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--process_outputs_function', choices = supported.process_outputs_functions)\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int, help='If eval_only is set, then eval_epoch allows you to specify evaluating at a particular epoch. By default, it evaluates the best epoch by validation performance.')\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, nargs='+', default=[0])\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_pred', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False, help='Whether to resume from the most recent saved model in the current log_dir.')\n",
    "\n",
    "# Weights & Biases\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--wandb_api_key_path', type=str,\n",
    "                    help=\"Path to Weights & Biases API Key. If use_wandb is set to True and this argument is not specified, user will be prompted to authenticate.\")\n",
    "parser.add_argument('--wandb_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for wandb.init() passed as key1=value1 key2=value2')\n",
    "\n",
    "# BREEDS\n",
    "parser.add_argument('--breeds', type=parse_bool, const=True, nargs='?', default=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f31678d-ebef-491d-adb0-4583f6712ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config(parser, dataset, algorithm):\n",
    "    global config, logger, mode\n",
    "    print(f'|   Updating config to use algorithm {algorithm}')\n",
    "    config = parser.parse_args((f'--dataset {dataset} '\n",
    "                            f'--algorithm {algorithm} ' \n",
    "                            '--root_dir /dccstor/hoo-misha-1/wilds/wilds/data '\n",
    "                            f'--wasserstein_blur 0.000001 '\n",
    "                            f'--coral_penalty_weight 0.0001 '\n",
    "                            f'--log_dir /dccstor/hoo-misha-1/wilds/WOODS/logs/cifar100/{algorithm} '\n",
    "                            f'--breeds True '\n",
    "                            f'--evaluate_all_splits False '\n",
    "                            #'--eval_only '\n",
    "                            #'--model_kwargs ignore_mismatched_sizes=True ' \n",
    "                            #'--evaluate_all_splits False '\n",
    "                            #'--use_wandb '\n",
    "                            ).split())\n",
    "    config = populate_defaults(config)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if len(config.device) > device_count:\n",
    "            raise ValueError(f\"Specified {len(config.device)} devices, but only {device_count} devices found.\")\n",
    "\n",
    "        config.use_data_parallel = len(config.device) > 1\n",
    "        device_str = \",\".join(map(str, config.device))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_str\n",
    "        config.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        config.use_data_parallel = False\n",
    "        config.device =torch.device(\"cpu\")\n",
    "    \n",
    "    # Initialize logs\n",
    "    if os.path.exists(config.log_dir) and config.resume:\n",
    "        resume=True\n",
    "        mode='a'\n",
    "    elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "        resume=False\n",
    "        mode='a'\n",
    "    else:\n",
    "        resume=False\n",
    "        mode='w'\n",
    "\n",
    "    if not os.path.exists(config.log_dir):\n",
    "        os.makedirs(config.log_dir)\n",
    "    logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d22d06d-a0c1-438d-a2a7-91400c7c5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Updating config to use algorithm wassersteindeepCORAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.Logger at 0x152f48f3a5e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_config(parser, 'iwildcam', 'wassersteindeepCORAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63fc6ac-d899-4b7e-85cc-67d01b7b525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "full_dataset = wilds.get_dataset(\n",
    "    dataset=config.dataset,\n",
    "    version=config.version,\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)\n",
    "\n",
    "# Transforms & data augmentations for labeled dataset\n",
    "# To modify data augmentation, modify the following code block.\n",
    "# If you want to use transforms that modify both `x` and `y`,\n",
    "# set `do_transform_y` to True when initializing the `WILDSSubset` below.\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset,\n",
    "    additional_transform_name=config.additional_train_transform,\n",
    "    is_training=True)\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset,\n",
    "    is_training=False)\n",
    "\n",
    "# Configure unlabeled datasets\n",
    "unlabeled_dataset = None\n",
    "if config.unlabeled_split is not None:\n",
    "    split = config.unlabeled_split\n",
    "    full_unlabeled_dataset = wilds.get_dataset(\n",
    "        dataset=config.dataset,\n",
    "        version=config.unlabeled_version,\n",
    "        root_dir=config.root_dir,\n",
    "        download=config.download,\n",
    "        unlabeled=True,\n",
    "        **config.dataset_kwargs\n",
    "    )\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=[full_dataset, full_unlabeled_dataset],\n",
    "        groupby_fields=config.groupby_fields\n",
    "    )\n",
    "\n",
    "    # Transforms & data augmentations for unlabeled dataset\n",
    "    if config.algorithm == \"FixMatch\":\n",
    "        # For FixMatch, we need our loader to return batches in the form ((x_weak, x_strong), m)\n",
    "        # We do this by initializing a special transform function\n",
    "        unlabeled_train_transform = initialize_transform(\n",
    "            config.transform, config, full_dataset, is_training=True, additional_transform_name=\"fixmatch\"\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, use the same data augmentations as the labeled data.\n",
    "        unlabeled_train_transform = train_transform\n",
    "\n",
    "    if config.algorithm == \"NoisyStudent\":\n",
    "        # For Noisy Student, we need to first generate pseudolabels using the teacher\n",
    "        # and then prep the unlabeled dataset to return these pseudolabels in __getitem__\n",
    "        print(\"Inferring teacher pseudolabels for Noisy Student\")\n",
    "        assert config.teacher_model_path is not None\n",
    "        if not config.teacher_model_path.endswith(\".pth\"):\n",
    "            # Use the best model\n",
    "            config.teacher_model_path = os.path.join(\n",
    "                config.teacher_model_path,  f\"{config.dataset}_seed:{config.seed}_epoch:best_model.pth\"\n",
    "            )\n",
    "\n",
    "        d_out = infer_d_out(full_dataset, config)\n",
    "        teacher_model = initialize_model(config, d_out).to(config.device)\n",
    "        load(teacher_model, config.teacher_model_path, device=config.device)\n",
    "        # Infer teacher outputs on weakly augmented unlabeled examples in sequential order\n",
    "        weak_transform = initialize_transform(\n",
    "            transform_name=config.transform,\n",
    "            config=config,\n",
    "            dataset=full_dataset,\n",
    "            is_training=True,\n",
    "            additional_transform_name=\"weak\"\n",
    "        )\n",
    "        unlabeled_split_dataset = full_unlabeled_dataset.get_subset(split, transform=weak_transform, frac=config.frac)\n",
    "        sequential_loader = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=unlabeled_split_dataset,\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.unlabeled_batch_size,\n",
    "            **config.unlabeled_loader_kwargs\n",
    "        )\n",
    "        teacher_outputs = infer_predictions(teacher_model, sequential_loader, config)\n",
    "        teacher_outputs = move_to(teacher_outputs, torch.device(\"cpu\"))\n",
    "        unlabeled_split_dataset = WILDSPseudolabeledSubset(\n",
    "            reference_subset=unlabeled_split_dataset,\n",
    "            pseudolabels=teacher_outputs,\n",
    "            transform=unlabeled_train_transform,\n",
    "            collate=full_dataset.collate,\n",
    "        )\n",
    "        teacher_model = teacher_model.to(torch.device(\"cpu\"))\n",
    "        del teacher_model\n",
    "    else:\n",
    "        unlabeled_split_dataset = full_unlabeled_dataset.get_subset(\n",
    "            split, \n",
    "            transform=unlabeled_train_transform, \n",
    "            frac=config.frac, \n",
    "            load_y=config.use_unlabeled_y\n",
    "        )\n",
    "\n",
    "    unlabeled_dataset = {\n",
    "        'split': split,\n",
    "        'name': full_unlabeled_dataset.split_names[split],\n",
    "        'dataset': unlabeled_split_dataset\n",
    "    }\n",
    "    unlabeled_dataset['loader'] = get_train_loader(\n",
    "        loader=config.train_loader,\n",
    "        dataset=unlabeled_dataset['dataset'],\n",
    "        batch_size=config.unlabeled_batch_size,\n",
    "        uniform_over_groups=config.uniform_over_groups,\n",
    "        grouper=train_grouper,\n",
    "        distinct_groups=config.distinct_groups,\n",
    "        n_groups_per_batch=config.unlabeled_n_groups_per_batch,\n",
    "        **config.unlabeled_loader_kwargs\n",
    "    )\n",
    "else:\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=config.groupby_fields\n",
    "    )\n",
    "\n",
    "# Configure labeled torch datasets (WILDS dataset splits)\n",
    "wilds_datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    wilds_datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        wilds_datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=wilds_datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        wilds_datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=wilds_datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    wilds_datasets[split]['split'] = split\n",
    "    wilds_datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    wilds_datasets[split]['verbose'] = verbose\n",
    "\n",
    "    # Loggers\n",
    "    wilds_datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=config.use_wandb\n",
    "    )\n",
    "    wilds_datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=config.use_wandb\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6caff5-e805-499f-be26-884944c90863",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['train']['dataset']._n_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8110d58-5d80-4a00-859c-72b7bb405915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize algorithm & load pretrained weights if provided\n",
    "algorithm = initialize_algorithm(\n",
    "    config=config,\n",
    "    datasets=wilds_datasets,\n",
    "    train_grouper=train_grouper,\n",
    "    unlabeled_dataset=unlabeled_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d565749b-d69c-4e2b-b99a-7643b73472fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from scripts.datasets import CustomCIFAR100\n",
    "\n",
    "from PIL import Image as im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced50485-3cba-4b4f-82b5-9030843cb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))\n",
    "train_transform = tt.Compose([\n",
    "    tt.RandomHorizontalFlip(),\n",
    "    tt.RandomCrop(32,padding=4,padding_mode=\"reflect\"),\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179300a9-412e-4702-8949-a86d9395d797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR100(root='/dccstor/hoo-misha-1/wilds/WOODS/data/', download=True, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb2ee8f-dbdf-4c76-b9fc-48abf9c9ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_coarse_fine = {\n",
    "    'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "    'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "    'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "    'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "    'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear',\n",
    "                             'sweet_pepper'],\n",
    "    'household electrical device': ['clock', 'keyboard', 'lamp',\n",
    "                                    'telephone', 'television'],\n",
    "    'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "    'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "    'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "    'large man-made outdoor things': ['bridge', 'castle', 'house', 'road',\n",
    "                                      'skyscraper'],\n",
    "    'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain',\n",
    "                                     'sea'],\n",
    "    'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee',\n",
    "                                       'elephant', 'kangaroo'],\n",
    "    'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "    'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "    'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "    'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "    'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree',\n",
    "              'willow_tree'],\n",
    "    'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "    'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor'],\n",
    "}\n",
    "\n",
    "mapping_coarse_idx = {\n",
    "    'aquatic mammals': 0,\n",
    "    'fish': 1,\n",
    "    'flowers': 2,\n",
    "    'food containers': 3,\n",
    "    'fruit and vegetables': 4,\n",
    "    'household electrical device': 5,\n",
    "    'household furniture': 6,\n",
    "    'insects': 7,\n",
    "    'large carnivores': 8,\n",
    "    'large man-made outdoor things': 9,\n",
    "    'large natural outdoor scenes': 10,\n",
    "    'large omnivores and herbivores': 11,\n",
    "    'medium-sized mammals': 12,\n",
    "    'non-insect invertebrates': 13,\n",
    "    'people': 14,\n",
    "    'reptiles': 15,\n",
    "    'small mammals': 16,\n",
    "    'trees': 17,\n",
    "    'vehicles 1': 18,\n",
    "    'vehicles 2': 19,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d804a512-8817-4ee3-9b76-d9844115e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = len(mapping_coarse_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9e0c67-f27c-494c-80c6-9c7b4d67f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cdbae5b-1933-43a1-81eb-9898ed6a168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_fine_domain = {}\n",
    "mapping_fine_coarse = {}\n",
    "mapping_domain_fine = {}\n",
    "mapping_idx_domain = [-1 for i in dataset.classes]\n",
    "for coarse_label in mapping_coarse_fine:\n",
    "    fine_labels = mapping_coarse_fine[coarse_label]\n",
    "    domain = 0\n",
    "    for fine_label in fine_labels:\n",
    "        mapping_fine_coarse[fine_label] = coarse_label\n",
    "        idx = dataset.classes.index(fine_label)\n",
    "        mapping_idx_domain[idx] = domain\n",
    "        mapping_fine_domain[fine_label] = domain\n",
    "        if domain in mapping_domain_fine:\n",
    "            mapping_domain_fine[domain].append(fine_label)\n",
    "        else:\n",
    "            mapping_domain_fine[domain] = [fine_label]\n",
    "        domain += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60da7af7-2f98-4e70-8355-32e512f2af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_targets = np.array([mapping_coarse_idx[mapping_fine_coarse[dataset.classes[idx]]] for idx in dataset.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80619b65-e009-4e07-ba45-8098deee63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = np.array([mapping_idx_domain[l] for l in dataset.targets])[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ad97fd2-8bc9-4854-bc58-5abe3d4ccdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acf0fff-81fd-4be3-9ff5-73c7b84929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_domain_idx = metadata < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97512b90-08bf-4248-9163-2d828ada2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_domain_idx = np.where(target_domain_idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f52154-69a3-4e06-bcbc-cdc56904b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_dataset = CustomCIFAR100(dataset.data[target_domain_idx], coarse_targets[target_domain_idx], metadata[target_domain_idx], dataset.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c65e2ac5-c66a-47e4-9683-98e9240da48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "train_dl = DataLoader(cifar100_dataset,BATCH_SIZE,num_workers=4,pin_memory=True,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c574cc5-eafc-4373-839d-fd19084abd70",
   "metadata": {},
   "source": [
    "# Replace WILDS dataset with BREEDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b39820d9-1545-42ed-bf6a-81fd1bab06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['train']['loader'] = train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5652fa38-e14e-4803-99cb-1e06e17484e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['train']['dataset']._n_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf7629b8-83eb-47b8-8a4b-a0faa0068b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['val']['loader'] = train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab3eac2-2160-4102-a909-06fffe8ec31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['val']['dataset']._n_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dab6572a-45c2-4eb1-8d84-7dbb6e6dcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from robustness.model_utils import make_and_restore_model\n",
    "# model, _ = make_and_restore_model(arch='resnet50', dataset =dataset_source)\n",
    "# model = model.model\n",
    "# model.needs_y = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "286879fa-d285-44ac-8f58-ed5f833a0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73b707fd-0b8e-457c-89c8-0b9bfb3dddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepCORAL\n",
    "if config.algorithm == 'deepCORAL':\n",
    "    algorithm.model = nn.Sequential(next(algorithm.model.children()), nn.Linear(2048,out_dim).to('cuda'))\n",
    "# wassersteindeepCORAL\n",
    "if config.algorithm == 'wassersteindeepCORAL':\n",
    "    algorithm.model = nn.Sequential(next(algorithm.model.children()), nn.Linear(2048,out_dim).to('cuda'))\n",
    "# ERM\n",
    "elif config.algorithm == 'ERM': \n",
    "    algorithm.model.fc = nn.Linear(2048, out_dim).to('cuda')\n",
    "# DANN\n",
    "elif config.algorithm == 'DANN':\n",
    "    algorithm.model.classifier = nn.Linear(2048,out_dim).to('cuda')\n",
    "    new_domain_classifier = []\n",
    "    count = 0\n",
    "    children = iter(algorithm.model.domain_classifier.children())\n",
    "    while count != 6:\n",
    "        new_domain_classifier.append(next(children))\n",
    "        count += 1\n",
    "    new_domain_classifier.append(nn.Linear(1024, domain_dim))\n",
    "    algorithm.model.domain_classifier = nn.Sequential(*new_domain_classifier).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5437ac15-96d7-4a20-8f9d-36dd5117cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_domain_classifier = []\n",
    "# count = 0\n",
    "# children = iter(algorithm.model.domain_classifier.children())\n",
    "# while count != 6:\n",
    "#     new_domain_classifier.append(next(children))\n",
    "#     count += 1\n",
    "# # new_domain_classifier.append(nn.Linear(1024, 10))\n",
    "# algorithm.model.domain_classifier = nn.Sequential(*new_domain_classifier).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5d432cc-5ba7-42cb-ad70-1cd2d8e5b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.no_group_logging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "869d4586-5161-4459-bcac-7bbb5d55116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.loss = nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8b1f5-7e07-4f36-b8fe-67d12496c6bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0]:\n",
      "\n",
      "Train:\n",
      "objective: 2.921\n",
      "penalty: 753.734\n",
      "loss_avg: 2.845\n",
      "acc_avg: 0.140\n",
      "\n",
      "objective: 2.476\n",
      "penalty: 724.743\n",
      "loss_avg: 2.404\n",
      "acc_avg: 0.307\n",
      "\n",
      "objective: 2.116\n",
      "penalty: 700.258\n",
      "loss_avg: 2.046\n",
      "acc_avg: 0.393\n",
      "\n",
      "objective: 1.872\n",
      "penalty: 661.365\n",
      "loss_avg: 1.806\n",
      "acc_avg: 0.447\n",
      "\n",
      "objective: 1.709\n",
      "penalty: 612.962\n",
      "loss_avg: 1.647\n",
      "acc_avg: 0.484\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.345\n",
      "Recall macro: 0.345\n",
      "F1 macro: 0.340\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 1.484\n",
      "penalty: 0.000\n",
      "loss_avg: 1.484\n",
      "acc_avg: 0.542\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.542\n",
      "Recall macro: 0.542\n",
      "F1 macro: 0.536\n",
      "Validation F1-macro_all: 0.536\n",
      "Epoch 0 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [1]:\n",
      "\n",
      "Train:\n",
      "objective: 1.569\n",
      "penalty: 592.958\n",
      "loss_avg: 1.509\n",
      "acc_avg: 0.527\n",
      "\n",
      "objective: 1.449\n",
      "penalty: 559.500\n",
      "loss_avg: 1.393\n",
      "acc_avg: 0.565\n",
      "\n",
      "objective: 1.394\n",
      "penalty: 509.336\n",
      "loss_avg: 1.343\n",
      "acc_avg: 0.579\n",
      "\n",
      "objective: 1.321\n",
      "penalty: 463.831\n",
      "loss_avg: 1.274\n",
      "acc_avg: 0.601\n",
      "\n",
      "objective: 1.313\n",
      "penalty: 432.770\n",
      "loss_avg: 1.270\n",
      "acc_avg: 0.595\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.572\n",
      "Recall macro: 0.572\n",
      "F1 macro: 0.570\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 1.128\n",
      "penalty: 0.000\n",
      "loss_avg: 1.128\n",
      "acc_avg: 0.644\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.644\n",
      "Recall macro: 0.644\n",
      "F1 macro: 0.643\n",
      "Validation F1-macro_all: 0.643\n",
      "Epoch 1 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [2]:\n",
      "\n",
      "Train:\n",
      "objective: 1.197\n",
      "penalty: 409.357\n",
      "loss_avg: 1.156\n",
      "acc_avg: 0.634\n",
      "\n",
      "objective: 1.185\n",
      "penalty: 387.085\n",
      "loss_avg: 1.146\n",
      "acc_avg: 0.640\n",
      "\n",
      "objective: 1.150\n",
      "penalty: 365.016\n",
      "loss_avg: 1.113\n",
      "acc_avg: 0.642\n",
      "\n",
      "objective: 1.128\n",
      "penalty: 345.380\n",
      "loss_avg: 1.094\n",
      "acc_avg: 0.657\n",
      "\n",
      "objective: 1.122\n",
      "penalty: 317.091\n",
      "loss_avg: 1.090\n",
      "acc_avg: 0.654\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.645\n",
      "Recall macro: 0.645\n",
      "F1 macro: 0.643\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.932\n",
      "penalty: 0.000\n",
      "loss_avg: 0.932\n",
      "acc_avg: 0.702\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.702\n",
      "Recall macro: 0.702\n",
      "F1 macro: 0.701\n",
      "Validation F1-macro_all: 0.701\n",
      "Epoch 2 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [3]:\n",
      "\n",
      "Train:\n",
      "objective: 0.997\n",
      "penalty: 331.246\n",
      "loss_avg: 0.964\n",
      "acc_avg: 0.696\n",
      "\n",
      "objective: 1.008\n",
      "penalty: 314.155\n",
      "loss_avg: 0.976\n",
      "acc_avg: 0.687\n",
      "\n",
      "objective: 0.986\n",
      "penalty: 306.308\n",
      "loss_avg: 0.955\n",
      "acc_avg: 0.689\n",
      "\n",
      "objective: 0.976\n",
      "penalty: 291.856\n",
      "loss_avg: 0.947\n",
      "acc_avg: 0.694\n",
      "\n",
      "objective: 0.971\n",
      "penalty: 272.984\n",
      "loss_avg: 0.943\n",
      "acc_avg: 0.697\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.692\n",
      "Recall macro: 0.692\n",
      "F1 macro: 0.691\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.810\n",
      "penalty: 0.000\n",
      "loss_avg: 0.810\n",
      "acc_avg: 0.740\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.739\n",
      "Recall macro: 0.739\n",
      "F1 macro: 0.739\n",
      "Validation F1-macro_all: 0.739\n",
      "Epoch 3 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [4]:\n",
      "\n",
      "Train:\n",
      "objective: 0.891\n",
      "penalty: 274.833\n",
      "loss_avg: 0.864\n",
      "acc_avg: 0.719\n",
      "\n",
      "objective: 0.891\n",
      "penalty: 278.613\n",
      "loss_avg: 0.864\n",
      "acc_avg: 0.727\n",
      "\n",
      "objective: 0.910\n",
      "penalty: 261.256\n",
      "loss_avg: 0.884\n",
      "acc_avg: 0.713\n",
      "\n",
      "objective: 0.882\n",
      "penalty: 259.238\n",
      "loss_avg: 0.856\n",
      "acc_avg: 0.725\n",
      "\n",
      "objective: 0.870\n",
      "penalty: 246.672\n",
      "loss_avg: 0.845\n",
      "acc_avg: 0.734\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.723\n",
      "Recall macro: 0.723\n",
      "F1 macro: 0.722\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.698\n",
      "penalty: 0.000\n",
      "loss_avg: 0.698\n",
      "acc_avg: 0.775\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.775\n",
      "Recall macro: 0.775\n",
      "F1 macro: 0.775\n",
      "Validation F1-macro_all: 0.775\n",
      "Epoch 4 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [5]:\n",
      "\n",
      "Train:\n",
      "objective: 0.764\n",
      "penalty: 255.434\n",
      "loss_avg: 0.738\n",
      "acc_avg: 0.759\n",
      "\n",
      "objective: 0.793\n",
      "penalty: 247.634\n",
      "loss_avg: 0.768\n",
      "acc_avg: 0.752\n",
      "\n",
      "objective: 0.805\n",
      "penalty: 244.583\n",
      "loss_avg: 0.780\n",
      "acc_avg: 0.750\n",
      "\n",
      "objective: 0.794\n",
      "penalty: 235.509\n",
      "loss_avg: 0.771\n",
      "acc_avg: 0.755\n",
      "\n",
      "objective: 0.768\n",
      "penalty: 234.309\n",
      "loss_avg: 0.744\n",
      "acc_avg: 0.760\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.755\n",
      "Recall macro: 0.755\n",
      "F1 macro: 0.755\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.627\n",
      "penalty: 0.000\n",
      "loss_avg: 0.627\n",
      "acc_avg: 0.797\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.797\n",
      "Recall macro: 0.797\n",
      "F1 macro: 0.797\n",
      "Validation F1-macro_all: 0.797\n",
      "Epoch 5 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [6]:\n",
      "\n",
      "Train:\n",
      "objective: 0.732\n",
      "penalty: 235.732\n",
      "loss_avg: 0.708\n",
      "acc_avg: 0.762\n",
      "\n",
      "objective: 0.695\n",
      "penalty: 235.555\n",
      "loss_avg: 0.672\n",
      "acc_avg: 0.784\n",
      "\n",
      "objective: 0.700\n",
      "penalty: 227.541\n",
      "loss_avg: 0.677\n",
      "acc_avg: 0.781\n",
      "\n",
      "objective: 0.726\n",
      "penalty: 220.847\n",
      "loss_avg: 0.704\n",
      "acc_avg: 0.773\n",
      "\n",
      "objective: 0.754\n",
      "penalty: 213.642\n",
      "loss_avg: 0.733\n",
      "acc_avg: 0.764\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.773\n",
      "Recall macro: 0.773\n",
      "F1 macro: 0.773\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.568\n",
      "penalty: 0.000\n",
      "loss_avg: 0.568\n",
      "acc_avg: 0.817\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.817\n",
      "Recall macro: 0.817\n",
      "F1 macro: 0.817\n",
      "Validation F1-macro_all: 0.817\n",
      "Epoch 6 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [7]:\n",
      "\n",
      "Train:\n",
      "objective: 0.648\n",
      "penalty: 214.962\n",
      "loss_avg: 0.626\n",
      "acc_avg: 0.797\n",
      "\n",
      "objective: 0.651\n",
      "penalty: 217.253\n",
      "loss_avg: 0.630\n",
      "acc_avg: 0.793\n",
      "\n",
      "objective: 0.664\n",
      "penalty: 210.013\n",
      "loss_avg: 0.643\n",
      "acc_avg: 0.791\n",
      "\n",
      "objective: 0.636\n",
      "penalty: 213.901\n",
      "loss_avg: 0.615\n",
      "acc_avg: 0.804\n",
      "\n",
      "objective: 0.677\n",
      "penalty: 210.563\n",
      "loss_avg: 0.656\n",
      "acc_avg: 0.787\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.795\n",
      "Recall macro: 0.795\n",
      "F1 macro: 0.795\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.493\n",
      "penalty: 0.000\n",
      "loss_avg: 0.493\n",
      "acc_avg: 0.841\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.841\n",
      "Recall macro: 0.841\n",
      "F1 macro: 0.840\n",
      "Validation F1-macro_all: 0.840\n",
      "Epoch 7 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [8]:\n",
      "\n",
      "Train:\n",
      "objective: 0.601\n",
      "penalty: 205.893\n",
      "loss_avg: 0.581\n",
      "acc_avg: 0.806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(algorithm, wilds_datasets, logger, config, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e65c5-9825-4855-95b2-f0d4e9b3811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde53bf-f4b7-4f02-9e60-9d9a959bfe2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3da62-845c-4a30-ae65-35f6419ed888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
