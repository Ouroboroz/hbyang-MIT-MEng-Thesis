{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f8e3b6-c555-48b9-96c3-75b77ca1a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/dccstor/hoo-misha-1/wilds/wilds/examples')\n",
    "sys.path.append('/dccstor/hoo-misha-1/wilds/WOODS')\n",
    "sys.path.append('/dccstor/hoo-misha-1/wilds/wilds')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "from models.bert.distilbert import DistilBertClassifier, DistilBertFeaturizer\n",
    "from configs.datasets import dataset_defaults\n",
    "\n",
    "import wilds\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.datasets.unlabeled.wilds_unlabeled_dataset import WILDSPseudolabeledSubset\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool, get_model_prefix, move_to\n",
    "from train import train, evaluate, infer_predictions,run_epoch\n",
    "from algorithms.initializer import initialize_algorithm, infer_d_out\n",
    "from transforms import initialize_transform\n",
    "\n",
    "from models.initializer import initialize_model\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported\n",
    "\n",
    "import torch.multiprocessing\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from examples.transforms import initialize_bert_transform\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import copy\n",
    "import re\n",
    "import psutil\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8253858-80a5-429d-97c2-a82ddb3dfc93",
   "metadata": {},
   "source": [
    "# Initialize Wilds Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4d13b4-e08c-444e-8e57-687ee303c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--breeds'], dest='breeds', nargs='?', const=True, default=False, type=<function parse_bool at 0x145c4fb43160>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Arg defaults are filled in according to examples/configs/ '''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required arguments\n",
    "parser.add_argument('-d', '--dataset', choices=wilds.supported_datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for dataset initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                    help='If true, tries to download the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes. Note that this also scales the test set down, so the reported numbers are not comparable with the full test set.')\n",
    "parser.add_argument('--version', default=None, type=str, help='WILDS labeled dataset version number.')\n",
    "\n",
    "# Unlabeled Dataset\n",
    "parser.add_argument('--unlabeled_split', default=None, type=str, choices=wilds.unlabeled_splits,  help='Unlabeled split to use. Some datasets only have some splits available.')\n",
    "parser.add_argument('--unlabeled_version', default=None, type=str, help='WILDS unlabeled dataset version number.')\n",
    "parser.add_argument('--use_unlabeled_y', default=False, type=parse_bool, const=True, nargs='?', \n",
    "                    help='If true, unlabeled loaders will also the true labels for the unlabeled data. This is only available for some datasets. Used for \"fully-labeled ERM experiments\" in the paper. Correct functionality relies on CrossEntropyLoss using ignore_index=-100.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--unlabeled_loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?', help='If true, sample examples such that batches are uniform over groups.')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?', help='If true, enforce groups sampled per batch are distinct.')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--unlabeled_n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--unlabeled_batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of batches to process before stepping optimizer and schedulers. If > 1, we simulate having a larger effective batch size (though batchnorm behaves differently).')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--noisystudent_add_dropout', type=parse_bool, const=True, nargs='?', help='If true, adds a dropout layer to the student model of NoisyStudent.')\n",
    "parser.add_argument('--noisystudent_dropout_rate', type=float)\n",
    "parser.add_argument('--pretrained_model_path', default=None, type=str, help='Specify a path to pretrained model weights')\n",
    "parser.add_argument('--load_featurizer_only', default=False, type=parse_bool, const=True, nargs='?', help='If true, only loads the featurizer weights and not the classifier weights.')\n",
    "\n",
    "# NoisyStudent-specific loading\n",
    "parser.add_argument('--teacher_model_path', type=str, help='Path to NoisyStudent teacher model weights. If this is defined, pseudolabels will first be computed for unlabeled data before anything else runs.')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--transform', choices=supported.transforms)\n",
    "parser.add_argument('--additional_train_transform', choices=supported.additional_transforms, help='Optional data augmentations to layer on top of the default transforms.')\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='The input resolution that images will be resized to before being passed into the model. For example, use --target_resolution 224 224 for a standard ResNet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "parser.add_argument('--randaugment_n', type=int, help='Number of RandAugment transformations to apply.')\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices=supported.losses)\n",
    "parser.add_argument('--loss_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for loss initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--wasserstein_blur', type=float, default=0.0001)\n",
    "parser.add_argument('--dann_penalty_weight', type=float)\n",
    "parser.add_argument('--dann_classifier_lr', type=float)\n",
    "parser.add_argument('--dann_featurizer_lr', type=float)\n",
    "parser.add_argument('--dann_discriminator_lr', type=float)\n",
    "parser.add_argument('--afn_penalty_weight', type=float)\n",
    "parser.add_argument('--safn_delta_r', type=float)\n",
    "parser.add_argument('--hafn_r', type=float)\n",
    "parser.add_argument('--use_hafn', default=False, type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--self_training_lambda', type=float)\n",
    "parser.add_argument('--self_training_threshold', type=float)\n",
    "parser.add_argument('--pseudolabel_T2', type=float, help='Percentage of total iterations at which to end linear scheduling and hold lambda at the max value')\n",
    "parser.add_argument('--soft_pseudolabels', default=False, type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--algo_log_metric')\n",
    "parser.add_argument('--process_pseudolabels_function', choices=supported.process_pseudolabels_functions)\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for optimizer initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for scheduler initialization passed as key1=value1 key2=value2')\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--process_outputs_function', choices = supported.process_outputs_functions)\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int, help='If eval_only is set, then eval_epoch allows you to specify evaluating at a particular epoch. By default, it evaluates the best epoch by validation performance.')\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, nargs='+', default=[0])\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_pred', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False, help='Whether to resume from the most recent saved model in the current log_dir.')\n",
    "\n",
    "# Weights & Biases\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--wandb_api_key_path', type=str,\n",
    "                    help=\"Path to Weights & Biases API Key. If use_wandb is set to True and this argument is not specified, user will be prompted to authenticate.\")\n",
    "parser.add_argument('--wandb_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "                    help='keyword arguments for wandb.init() passed as key1=value1 key2=value2')\n",
    "\n",
    "# BREEDS\n",
    "parser.add_argument('--breeds', type=parse_bool, const=True, nargs='?', default=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f31678d-ebef-491d-adb0-4583f6712ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config(parser, dataset, algorithm):\n",
    "    global config, logger, mode\n",
    "    print(f'|   Updating config to use algorithm {algorithm}')\n",
    "    config = parser.parse_args((f'--dataset {dataset} '\n",
    "                            f'--algorithm {algorithm} ' \n",
    "                            '--root_dir /dccstor/hoo-misha-1/wilds/wilds/data '\n",
    "                            f'--wasserstein_blur 0.000001 '\n",
    "                            f'--coral_penalty_weight 0.0001 '\n",
    "                            f'--log_dir /dccstor/hoo-misha-1/wilds/WOODS/logs/entity13/{algorithm} '\n",
    "                            f'--breeds True '\n",
    "                            f'--evaluate_all_splits False '\n",
    "                            #'--eval_only '\n",
    "                            #'--model_kwargs ignore_mismatched_sizes=True ' \n",
    "                            #'--evaluate_all_splits False '\n",
    "                            #'--use_wandb '\n",
    "                            ).split())\n",
    "    config = populate_defaults(config)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if len(config.device) > device_count:\n",
    "            raise ValueError(f\"Specified {len(config.device)} devices, but only {device_count} devices found.\")\n",
    "\n",
    "        config.use_data_parallel = len(config.device) > 1\n",
    "        device_str = \",\".join(map(str, config.device))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_str\n",
    "        config.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        config.use_data_parallel = False\n",
    "        config.device =torch.device(\"cpu\")\n",
    "    \n",
    "    # Initialize logs\n",
    "    if os.path.exists(config.log_dir) and config.resume:\n",
    "        resume=True\n",
    "        mode='a'\n",
    "    elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "        resume=False\n",
    "        mode='a'\n",
    "    else:\n",
    "        resume=False\n",
    "        mode='w'\n",
    "\n",
    "    if not os.path.exists(config.log_dir):\n",
    "        os.makedirs(config.log_dir)\n",
    "    logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d22d06d-a0c1-438d-a2a7-91400c7c5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Updating config to use algorithm ERM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.Logger at 0x145db9508e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_config(parser, 'iwildcam', 'ERM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63fc6ac-d899-4b7e-85cc-67d01b7b525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "full_dataset = wilds.get_dataset(\n",
    "    dataset=config.dataset,\n",
    "    version=config.version,\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)\n",
    "\n",
    "# Transforms & data augmentations for labeled dataset\n",
    "# To modify data augmentation, modify the following code block.\n",
    "# If you want to use transforms that modify both `x` and `y`,\n",
    "# set `do_transform_y` to True when initializing the `WILDSSubset` below.\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset,\n",
    "    additional_transform_name=config.additional_train_transform,\n",
    "    is_training=True)\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset,\n",
    "    is_training=False)\n",
    "\n",
    "# Configure unlabeled datasets\n",
    "unlabeled_dataset = None\n",
    "if config.unlabeled_split is not None:\n",
    "    split = config.unlabeled_split\n",
    "    full_unlabeled_dataset = wilds.get_dataset(\n",
    "        dataset=config.dataset,\n",
    "        version=config.unlabeled_version,\n",
    "        root_dir=config.root_dir,\n",
    "        download=config.download,\n",
    "        unlabeled=True,\n",
    "        **config.dataset_kwargs\n",
    "    )\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=[full_dataset, full_unlabeled_dataset],\n",
    "        groupby_fields=config.groupby_fields\n",
    "    )\n",
    "\n",
    "    # Transforms & data augmentations for unlabeled dataset\n",
    "    if config.algorithm == \"FixMatch\":\n",
    "        # For FixMatch, we need our loader to return batches in the form ((x_weak, x_strong), m)\n",
    "        # We do this by initializing a special transform function\n",
    "        unlabeled_train_transform = initialize_transform(\n",
    "            config.transform, config, full_dataset, is_training=True, additional_transform_name=\"fixmatch\"\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, use the same data augmentations as the labeled data.\n",
    "        unlabeled_train_transform = train_transform\n",
    "\n",
    "    if config.algorithm == \"NoisyStudent\":\n",
    "        # For Noisy Student, we need to first generate pseudolabels using the teacher\n",
    "        # and then prep the unlabeled dataset to return these pseudolabels in __getitem__\n",
    "        print(\"Inferring teacher pseudolabels for Noisy Student\")\n",
    "        assert config.teacher_model_path is not None\n",
    "        if not config.teacher_model_path.endswith(\".pth\"):\n",
    "            # Use the best model\n",
    "            config.teacher_model_path = os.path.join(\n",
    "                config.teacher_model_path,  f\"{config.dataset}_seed:{config.seed}_epoch:best_model.pth\"\n",
    "            )\n",
    "\n",
    "        d_out = infer_d_out(full_dataset, config)\n",
    "        teacher_model = initialize_model(config, d_out).to(config.device)\n",
    "        load(teacher_model, config.teacher_model_path, device=config.device)\n",
    "        # Infer teacher outputs on weakly augmented unlabeled examples in sequential order\n",
    "        weak_transform = initialize_transform(\n",
    "            transform_name=config.transform,\n",
    "            config=config,\n",
    "            dataset=full_dataset,\n",
    "            is_training=True,\n",
    "            additional_transform_name=\"weak\"\n",
    "        )\n",
    "        unlabeled_split_dataset = full_unlabeled_dataset.get_subset(split, transform=weak_transform, frac=config.frac)\n",
    "        sequential_loader = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=unlabeled_split_dataset,\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.unlabeled_batch_size,\n",
    "            **config.unlabeled_loader_kwargs\n",
    "        )\n",
    "        teacher_outputs = infer_predictions(teacher_model, sequential_loader, config)\n",
    "        teacher_outputs = move_to(teacher_outputs, torch.device(\"cpu\"))\n",
    "        unlabeled_split_dataset = WILDSPseudolabeledSubset(\n",
    "            reference_subset=unlabeled_split_dataset,\n",
    "            pseudolabels=teacher_outputs,\n",
    "            transform=unlabeled_train_transform,\n",
    "            collate=full_dataset.collate,\n",
    "        )\n",
    "        teacher_model = teacher_model.to(torch.device(\"cpu\"))\n",
    "        del teacher_model\n",
    "    else:\n",
    "        unlabeled_split_dataset = full_unlabeled_dataset.get_subset(\n",
    "            split, \n",
    "            transform=unlabeled_train_transform, \n",
    "            frac=config.frac, \n",
    "            load_y=config.use_unlabeled_y\n",
    "        )\n",
    "\n",
    "    unlabeled_dataset = {\n",
    "        'split': split,\n",
    "        'name': full_unlabeled_dataset.split_names[split],\n",
    "        'dataset': unlabeled_split_dataset\n",
    "    }\n",
    "    unlabeled_dataset['loader'] = get_train_loader(\n",
    "        loader=config.train_loader,\n",
    "        dataset=unlabeled_dataset['dataset'],\n",
    "        batch_size=config.unlabeled_batch_size,\n",
    "        uniform_over_groups=config.uniform_over_groups,\n",
    "        grouper=train_grouper,\n",
    "        distinct_groups=config.distinct_groups,\n",
    "        n_groups_per_batch=config.unlabeled_n_groups_per_batch,\n",
    "        **config.unlabeled_loader_kwargs\n",
    "    )\n",
    "else:\n",
    "    train_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=config.groupby_fields\n",
    "    )\n",
    "\n",
    "# Configure labeled torch datasets (WILDS dataset splits)\n",
    "wilds_datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    wilds_datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        wilds_datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=wilds_datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        wilds_datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=wilds_datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    wilds_datasets[split]['split'] = split\n",
    "    wilds_datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    wilds_datasets[split]['verbose'] = verbose\n",
    "\n",
    "    # Loggers\n",
    "    wilds_datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=config.use_wandb\n",
    "    )\n",
    "    wilds_datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=config.use_wandb\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8110d58-5d80-4a00-859c-72b7bb405915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize algorithm & load pretrained weights if provided\n",
    "algorithm = initialize_algorithm(\n",
    "    config=config,\n",
    "    datasets=wilds_datasets,\n",
    "    train_grouper=train_grouper,\n",
    "    unlabeled_dataset=unlabeled_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d565749b-d69c-4e2b-b99a-7643b73472fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness.tools.breeds_helpers import make_entity13\n",
    "from robustness.tools.breeds_helpers import ClassHierarchy\n",
    "#from robustness import datasets\n",
    "from scripts import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9da52c-80d6-4fb9-bb01-73c9e698f3cf",
   "metadata": {},
   "source": [
    "# Initialize BREEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2967eca8-d2d2-4f3c-acef-febe69da5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/dccstor/leonidka1/data/imagenet/ILSVRC/Data/CLS-LOC/'\n",
    "info_dir = '/dccstor/hoo-misha-1/wilds/BREEDS-Benchmarks/imagenet_class_hierarchy/modified'\n",
    "num_workers = 8\n",
    "batch_size = 200\n",
    "\n",
    "hier = ClassHierarchy(info_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54bb691-cbc3-479e-b248-aa65c63e206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = make_entity13(info_dir, split=\"rand\")\n",
    "superclasses, subclass_split, label_map = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb2ee8f-dbdf-4c76-b9fc-48abf9c9ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subclasses, test_subclasses = subclass_split\n",
    "train_subclasses = np.array(train_subclasses)\n",
    "test_subclasses = np.array(test_subclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cdbae5b-1933-43a1-81eb-9898ed6a168c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[869, 399, 578, 735, 652, 610, 501, 445, 655, 842],\n",
       "       [ 87,  92,  91, 137,  14, 145, 129, 135,  85,  21],\n",
       "       [ 45,  42,  51,  47,  52,  63,  55,  35,  58,  33],\n",
       "       [119,  72, 300,  75,  73, 317, 302, 120, 309, 313],\n",
       "       [284, 350, 292, 344, 174, 149, 375, 258, 283, 170],\n",
       "       [443, 552, 824, 728, 433, 514, 679, 518, 570, 638],\n",
       "       [484, 814, 554, 914, 404, 510, 628, 871, 812, 403],\n",
       "       [890, 681, 430, 590, 872, 745, 422, 664, 522, 416],\n",
       "       [894, 861, 553, 669, 564, 431, 493, 559, 556, 789],\n",
       "       [881, 695, 626, 822, 704, 499, 845, 398, 778, 512],\n",
       "       [483, 442, 562, 727, 920, 460, 500, 663, 743, 900],\n",
       "       [803, 867, 751, 791, 880, 670, 705, 654, 609, 757],\n",
       "       [937, 987, 950, 943, 940, 942, 941, 938, 945, 952]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3a98e9-809c-49d8-b2f7-a3e87e609b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(train_subclasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6bb724a-a0bc-4f33-8ceb-ace12141ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subdomains_subclasses = {}\n",
    "test_subdomains_subclasses = {}\n",
    "for i in range(train_subclasses.shape[1]):\n",
    "    for subclass in train_subclasses[:,i]:\n",
    "        train_subdomains_subclasses[subclass] = torch.tensor([i,i]).to('cpu')\n",
    "    \n",
    "for i in range(test_subclasses.shape[1]//2):\n",
    "    test_subdomains_subclasses[str((i,))] = test_subclasses[:,[i]].tolist()\n",
    "    test_subdomains_subclasses[str((i+1,))] = test_subclasses[:,[i+1]].tolist()\n",
    "    test_subdomains_subclasses[str((i,i+1))] = test_subclasses[:,i:i+2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65e2ac5-c66a-47e4-9683-98e9240da48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset custom_imagenet..\n"
     ]
    }
   ],
   "source": [
    "dataset_source = datasets.WILDSCustomImageNet(data_dir, train_subclasses, train_subdomains_subclasses)\n",
    "loaders_source = dataset_source.make_loaders(num_workers, batch_size)\n",
    "train_loader_source, val_loader_source = loaders_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c574cc5-eafc-4373-839d-fd19084abd70",
   "metadata": {},
   "source": [
    "# Replace WILDS dataset with BREEDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39820d9-1545-42ed-bf6a-81fd1bab06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['train']['loader'] = train_loader_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf7629b8-83eb-47b8-8a4b-a0faa0068b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilds_datasets['val']['loader'] = val_loader_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab6572a-45c2-4eb1-8d84-7dbb6e6dcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from robustness.model_utils import make_and_restore_model\n",
    "# model, _ = make_and_restore_model(arch='resnet50', dataset =dataset_source)\n",
    "# model = model.model\n",
    "# model.needs_y = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "286879fa-d285-44ac-8f58-ed5f833a0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30cbd7f6-da51-4006-9960-4bc7a0108a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=182, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73b707fd-0b8e-457c-89c8-0b9bfb3dddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepCORAL\n",
    "if config.algorithm == 'deepCORAL':\n",
    "    algorithm.model = nn.Sequential(next(algorithm.model.children()), nn.Linear(2048,13).to('cuda'))\n",
    "# wassersteindeepCORAL\n",
    "if config.algorithm == 'wassersteindeepCORAL':\n",
    "    algorithm.model = nn.Sequential(next(algorithm.model.children()), nn.Linear(2048,13).to('cuda'))\n",
    "# ERM\n",
    "elif config.algorithm == 'ERM': \n",
    "    algorithm.model.fc = nn.Linear(2048, 13).to('cuda')\n",
    "# DANN\n",
    "elif config.algorithm == 'DANN':\n",
    "    algorithm.model.classifier = nn.Linear(2048,13).to('cuda')\n",
    "    new_domain_classifier = []\n",
    "    count = 0\n",
    "    children = iter(algorithm.model.domain_classifier.children())\n",
    "    while count != 6:\n",
    "        new_domain_classifier.append(next(children))\n",
    "        count += 1\n",
    "    # new_domain_classifier.append(nn.Linear(1024, 10))\n",
    "    algorithm.model.domain_classifier = nn.Sequential(*new_domain_classifier).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5437ac15-96d7-4a20-8f9d-36dd5117cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_domain_classifier = []\n",
    "# count = 0\n",
    "# children = iter(algorithm.model.domain_classifier.children())\n",
    "# while count != 6:\n",
    "#     new_domain_classifier.append(next(children))\n",
    "#     count += 1\n",
    "# # new_domain_classifier.append(nn.Linear(1024, 10))\n",
    "# algorithm.model.domain_classifier = nn.Sequential(*new_domain_classifier).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5d432cc-5ba7-42cb-ad70-1cd2d8e5b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.no_group_logging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "869d4586-5161-4459-bcac-7bbb5d55116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm.loss = nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809207f6-5d75-4e0b-b6f6-bd835b257029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3e8b1f5-7e07-4f36-b8fe-67d12496c6bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0]:\n",
      "\n",
      "Train:\n",
      "objective: 1.665\n",
      "loss_avg: 1.665\n",
      "acc_avg: 0.579\n",
      "\n",
      "objective: 0.722\n",
      "loss_avg: 0.722\n",
      "acc_avg: 0.809\n",
      "\n",
      "objective: 0.538\n",
      "loss_avg: 0.538\n",
      "acc_avg: 0.843\n",
      "\n",
      "objective: 0.448\n",
      "loss_avg: 0.448\n",
      "acc_avg: 0.868\n",
      "\n",
      "objective: 0.421\n",
      "loss_avg: 0.421\n",
      "acc_avg: 0.873\n",
      "\n",
      "objective: 0.394\n",
      "loss_avg: 0.394\n",
      "acc_avg: 0.878\n",
      "\n",
      "objective: 0.358\n",
      "loss_avg: 0.358\n",
      "acc_avg: 0.888\n",
      "\n",
      "objective: 0.310\n",
      "loss_avg: 0.310\n",
      "acc_avg: 0.906\n",
      "\n",
      "objective: 0.326\n",
      "loss_avg: 0.326\n",
      "acc_avg: 0.903\n",
      "\n",
      "objective: 0.317\n",
      "loss_avg: 0.317\n",
      "acc_avg: 0.902\n",
      "\n",
      "objective: 0.321\n",
      "loss_avg: 0.321\n",
      "acc_avg: 0.898\n",
      "\n",
      "objective: 0.319\n",
      "loss_avg: 0.319\n",
      "acc_avg: 0.903\n",
      "\n",
      "objective: 0.305\n",
      "loss_avg: 0.305\n",
      "acc_avg: 0.908\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.866\n",
      "Recall macro: 0.866\n",
      "F1 macro: 0.865\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.206\n",
      "loss_avg: 0.206\n",
      "acc_avg: 0.936\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.936\n",
      "Recall macro: 0.936\n",
      "F1 macro: 0.936\n",
      "Validation F1-macro_all: 0.936\n",
      "Epoch 0 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [1]:\n",
      "\n",
      "Train:\n",
      "objective: 0.275\n",
      "loss_avg: 0.275\n",
      "acc_avg: 0.915\n",
      "\n",
      "objective: 0.280\n",
      "loss_avg: 0.280\n",
      "acc_avg: 0.914\n",
      "\n",
      "objective: 0.277\n",
      "loss_avg: 0.277\n",
      "acc_avg: 0.914\n",
      "\n",
      "objective: 0.264\n",
      "loss_avg: 0.264\n",
      "acc_avg: 0.919\n",
      "\n",
      "objective: 0.264\n",
      "loss_avg: 0.264\n",
      "acc_avg: 0.917\n",
      "\n",
      "objective: 0.278\n",
      "loss_avg: 0.278\n",
      "acc_avg: 0.910\n",
      "\n",
      "objective: 0.278\n",
      "loss_avg: 0.278\n",
      "acc_avg: 0.916\n",
      "\n",
      "objective: 0.267\n",
      "loss_avg: 0.267\n",
      "acc_avg: 0.916\n",
      "\n",
      "objective: 0.261\n",
      "loss_avg: 0.261\n",
      "acc_avg: 0.918\n",
      "\n",
      "objective: 0.268\n",
      "loss_avg: 0.268\n",
      "acc_avg: 0.918\n",
      "\n",
      "objective: 0.261\n",
      "loss_avg: 0.261\n",
      "acc_avg: 0.918\n",
      "\n",
      "objective: 0.272\n",
      "loss_avg: 0.272\n",
      "acc_avg: 0.916\n",
      "\n",
      "objective: 0.249\n",
      "loss_avg: 0.249\n",
      "acc_avg: 0.922\n",
      "\n",
      "objective: 0.266\n",
      "loss_avg: 0.266\n",
      "acc_avg: 0.915\n",
      "\n",
      "objective: 0.279\n",
      "loss_avg: 0.279\n",
      "acc_avg: 0.912\n",
      "\n",
      "objective: 0.262\n",
      "loss_avg: 0.262\n",
      "acc_avg: 0.917\n",
      "\n",
      "objective: 0.273\n",
      "loss_avg: 0.273\n",
      "acc_avg: 0.910\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.916\n",
      "Recall macro: 0.915\n",
      "F1 macro: 0.915\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.178\n",
      "loss_avg: 0.178\n",
      "acc_avg: 0.944\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.944\n",
      "Recall macro: 0.944\n",
      "F1 macro: 0.944\n",
      "Validation F1-macro_all: 0.944\n",
      "Epoch 1 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [2]:\n",
      "\n",
      "Train:\n",
      "objective: 0.238\n",
      "loss_avg: 0.238\n",
      "acc_avg: 0.925\n",
      "\n",
      "objective: 0.249\n",
      "loss_avg: 0.249\n",
      "acc_avg: 0.923\n",
      "\n",
      "objective: 0.240\n",
      "loss_avg: 0.240\n",
      "acc_avg: 0.926\n",
      "\n",
      "objective: 0.239\n",
      "loss_avg: 0.239\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.238\n",
      "loss_avg: 0.238\n",
      "acc_avg: 0.926\n",
      "\n",
      "objective: 0.242\n",
      "loss_avg: 0.242\n",
      "acc_avg: 0.923\n",
      "\n",
      "objective: 0.251\n",
      "loss_avg: 0.251\n",
      "acc_avg: 0.921\n",
      "\n",
      "objective: 0.250\n",
      "loss_avg: 0.250\n",
      "acc_avg: 0.921\n",
      "\n",
      "objective: 0.245\n",
      "loss_avg: 0.245\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.236\n",
      "loss_avg: 0.236\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.239\n",
      "loss_avg: 0.239\n",
      "acc_avg: 0.922\n",
      "\n",
      "objective: 0.241\n",
      "loss_avg: 0.241\n",
      "acc_avg: 0.925\n",
      "\n",
      "objective: 0.242\n",
      "loss_avg: 0.242\n",
      "acc_avg: 0.923\n",
      "\n",
      "objective: 0.245\n",
      "loss_avg: 0.245\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.233\n",
      "loss_avg: 0.233\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.232\n",
      "loss_avg: 0.232\n",
      "acc_avg: 0.928\n",
      "\n",
      "objective: 0.259\n",
      "loss_avg: 0.259\n",
      "acc_avg: 0.914\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.924\n",
      "Recall macro: 0.923\n",
      "F1 macro: 0.923\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.184\n",
      "loss_avg: 0.184\n",
      "acc_avg: 0.944\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.944\n",
      "Recall macro: 0.944\n",
      "F1 macro: 0.943\n",
      "Validation F1-macro_all: 0.943\n",
      "\n",
      "\n",
      "Epoch [3]:\n",
      "\n",
      "Train:\n",
      "objective: 0.230\n",
      "loss_avg: 0.230\n",
      "acc_avg: 0.928\n",
      "\n",
      "objective: 0.214\n",
      "loss_avg: 0.214\n",
      "acc_avg: 0.933\n",
      "\n",
      "objective: 0.233\n",
      "loss_avg: 0.233\n",
      "acc_avg: 0.926\n",
      "\n",
      "objective: 0.213\n",
      "loss_avg: 0.213\n",
      "acc_avg: 0.934\n",
      "\n",
      "objective: 0.215\n",
      "loss_avg: 0.215\n",
      "acc_avg: 0.929\n",
      "\n",
      "objective: 0.214\n",
      "loss_avg: 0.214\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.213\n",
      "loss_avg: 0.213\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.230\n",
      "loss_avg: 0.230\n",
      "acc_avg: 0.926\n",
      "\n",
      "objective: 0.225\n",
      "loss_avg: 0.225\n",
      "acc_avg: 0.929\n",
      "\n",
      "objective: 0.221\n",
      "loss_avg: 0.221\n",
      "acc_avg: 0.930\n",
      "\n",
      "objective: 0.235\n",
      "loss_avg: 0.235\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.217\n",
      "loss_avg: 0.217\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.219\n",
      "loss_avg: 0.219\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.238\n",
      "loss_avg: 0.238\n",
      "acc_avg: 0.924\n",
      "\n",
      "objective: 0.218\n",
      "loss_avg: 0.218\n",
      "acc_avg: 0.930\n",
      "\n",
      "objective: 0.226\n",
      "loss_avg: 0.226\n",
      "acc_avg: 0.928\n",
      "\n",
      "objective: 0.210\n",
      "loss_avg: 0.210\n",
      "acc_avg: 0.932\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.929\n",
      "Recall macro: 0.929\n",
      "F1 macro: 0.929\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.165\n",
      "loss_avg: 0.165\n",
      "acc_avg: 0.947\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.947\n",
      "Recall macro: 0.947\n",
      "F1 macro: 0.947\n",
      "Validation F1-macro_all: 0.947\n",
      "Epoch 3 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [4]:\n",
      "\n",
      "Train:\n",
      "objective: 0.208\n",
      "loss_avg: 0.208\n",
      "acc_avg: 0.934\n",
      "\n",
      "objective: 0.201\n",
      "loss_avg: 0.201\n",
      "acc_avg: 0.936\n",
      "\n",
      "objective: 0.208\n",
      "loss_avg: 0.208\n",
      "acc_avg: 0.932\n",
      "\n",
      "objective: 0.224\n",
      "loss_avg: 0.224\n",
      "acc_avg: 0.928\n",
      "\n",
      "objective: 0.214\n",
      "loss_avg: 0.214\n",
      "acc_avg: 0.933\n",
      "\n",
      "objective: 0.218\n",
      "loss_avg: 0.218\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.205\n",
      "loss_avg: 0.205\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.215\n",
      "loss_avg: 0.215\n",
      "acc_avg: 0.936\n",
      "\n",
      "objective: 0.204\n",
      "loss_avg: 0.204\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.213\n",
      "loss_avg: 0.213\n",
      "acc_avg: 0.932\n",
      "\n",
      "objective: 0.197\n",
      "loss_avg: 0.197\n",
      "acc_avg: 0.937\n",
      "\n",
      "objective: 0.214\n",
      "loss_avg: 0.214\n",
      "acc_avg: 0.932\n",
      "\n",
      "objective: 0.215\n",
      "loss_avg: 0.215\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.213\n",
      "loss_avg: 0.213\n",
      "acc_avg: 0.931\n",
      "\n",
      "objective: 0.212\n",
      "loss_avg: 0.212\n",
      "acc_avg: 0.932\n",
      "\n",
      "objective: 0.207\n",
      "loss_avg: 0.207\n",
      "acc_avg: 0.934\n",
      "\n",
      "objective: 0.219\n",
      "loss_avg: 0.219\n",
      "acc_avg: 0.928\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.933\n",
      "Recall macro: 0.933\n",
      "F1 macro: 0.933\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.165\n",
      "loss_avg: 0.165\n",
      "acc_avg: 0.948\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.948\n",
      "Recall macro: 0.948\n",
      "F1 macro: 0.948\n",
      "Validation F1-macro_all: 0.948\n",
      "Epoch 4 has the best validation performance so far.\n",
      "\n",
      "\n",
      "Epoch [5]:\n",
      "\n",
      "Train:\n",
      "objective: 0.202\n",
      "loss_avg: 0.202\n",
      "acc_avg: 0.933\n",
      "\n",
      "objective: 0.194\n",
      "loss_avg: 0.194\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.199\n",
      "loss_avg: 0.199\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.203\n",
      "loss_avg: 0.203\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.194\n",
      "loss_avg: 0.194\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.208\n",
      "loss_avg: 0.208\n",
      "acc_avg: 0.933\n",
      "\n",
      "objective: 0.203\n",
      "loss_avg: 0.203\n",
      "acc_avg: 0.936\n",
      "\n",
      "objective: 0.208\n",
      "loss_avg: 0.208\n",
      "acc_avg: 0.933\n",
      "\n",
      "objective: 0.202\n",
      "loss_avg: 0.202\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.207\n",
      "loss_avg: 0.207\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.205\n",
      "loss_avg: 0.205\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.209\n",
      "loss_avg: 0.209\n",
      "acc_avg: 0.932\n",
      "\n",
      "objective: 0.207\n",
      "loss_avg: 0.207\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.210\n",
      "loss_avg: 0.210\n",
      "acc_avg: 0.934\n",
      "\n",
      "objective: 0.202\n",
      "loss_avg: 0.202\n",
      "acc_avg: 0.934\n",
      "\n",
      "objective: 0.204\n",
      "loss_avg: 0.204\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.212\n",
      "loss_avg: 0.212\n",
      "acc_avg: 0.929\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.935\n",
      "Recall macro: 0.934\n",
      "F1 macro: 0.934\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.162\n",
      "loss_avg: 0.162\n",
      "acc_avg: 0.948\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.948\n",
      "Recall macro: 0.948\n",
      "F1 macro: 0.948\n",
      "Validation F1-macro_all: 0.948\n",
      "\n",
      "\n",
      "Epoch [6]:\n",
      "\n",
      "Train:\n",
      "objective: 0.188\n",
      "loss_avg: 0.188\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.199\n",
      "loss_avg: 0.199\n",
      "acc_avg: 0.936\n",
      "\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.189\n",
      "loss_avg: 0.189\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.187\n",
      "loss_avg: 0.187\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.188\n",
      "loss_avg: 0.188\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.185\n",
      "loss_avg: 0.185\n",
      "acc_avg: 0.941\n",
      "\n",
      "objective: 0.202\n",
      "loss_avg: 0.202\n",
      "acc_avg: 0.936\n",
      "\n",
      "objective: 0.192\n",
      "loss_avg: 0.192\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.187\n",
      "loss_avg: 0.187\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.199\n",
      "loss_avg: 0.199\n",
      "acc_avg: 0.936\n",
      "\n",
      "objective: 0.197\n",
      "loss_avg: 0.197\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.205\n",
      "loss_avg: 0.205\n",
      "acc_avg: 0.935\n",
      "\n",
      "objective: 0.192\n",
      "loss_avg: 0.192\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.203\n",
      "loss_avg: 0.203\n",
      "acc_avg: 0.934\n",
      "\n",
      "objective: 0.178\n",
      "loss_avg: 0.178\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.191\n",
      "loss_avg: 0.191\n",
      "acc_avg: 0.938\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.939\n",
      "Recall macro: 0.939\n",
      "F1 macro: 0.939\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.162\n",
      "loss_avg: 0.162\n",
      "acc_avg: 0.947\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.947\n",
      "Recall macro: 0.947\n",
      "F1 macro: 0.947\n",
      "Validation F1-macro_all: 0.947\n",
      "\n",
      "\n",
      "Epoch [7]:\n",
      "\n",
      "Train:\n",
      "objective: 0.200\n",
      "loss_avg: 0.200\n",
      "acc_avg: 0.937\n",
      "\n",
      "objective: 0.178\n",
      "loss_avg: 0.178\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.179\n",
      "loss_avg: 0.179\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.182\n",
      "loss_avg: 0.182\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.186\n",
      "loss_avg: 0.186\n",
      "acc_avg: 0.941\n",
      "\n",
      "objective: 0.195\n",
      "loss_avg: 0.195\n",
      "acc_avg: 0.938\n",
      "\n",
      "objective: 0.185\n",
      "loss_avg: 0.185\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.188\n",
      "loss_avg: 0.188\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.175\n",
      "loss_avg: 0.175\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.197\n",
      "loss_avg: 0.197\n",
      "acc_avg: 0.938\n",
      "\n",
      "objective: 0.192\n",
      "loss_avg: 0.192\n",
      "acc_avg: 0.938\n",
      "\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.189\n",
      "loss_avg: 0.189\n",
      "acc_avg: 0.941\n",
      "\n",
      "objective: 0.185\n",
      "loss_avg: 0.185\n",
      "acc_avg: 0.941\n",
      "\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.187\n",
      "loss_avg: 0.187\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.190\n",
      "loss_avg: 0.190\n",
      "acc_avg: 0.941\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.941\n",
      "Recall macro: 0.941\n",
      "F1 macro: 0.941\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.162\n",
      "loss_avg: 0.162\n",
      "acc_avg: 0.948\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.948\n",
      "Recall macro: 0.948\n",
      "F1 macro: 0.948\n",
      "Validation F1-macro_all: 0.948\n",
      "\n",
      "\n",
      "Epoch [8]:\n",
      "\n",
      "Train:\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.178\n",
      "loss_avg: 0.178\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.184\n",
      "loss_avg: 0.184\n",
      "acc_avg: 0.940\n",
      "\n",
      "objective: 0.184\n",
      "loss_avg: 0.184\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.179\n",
      "loss_avg: 0.179\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.174\n",
      "loss_avg: 0.174\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.181\n",
      "loss_avg: 0.181\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.186\n",
      "loss_avg: 0.186\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.176\n",
      "loss_avg: 0.176\n",
      "acc_avg: 0.946\n",
      "\n",
      "objective: 0.177\n",
      "loss_avg: 0.177\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.177\n",
      "loss_avg: 0.177\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.173\n",
      "loss_avg: 0.173\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.190\n",
      "loss_avg: 0.190\n",
      "acc_avg: 0.938\n",
      "\n",
      "objective: 0.181\n",
      "loss_avg: 0.181\n",
      "acc_avg: 0.947\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.943\n",
      "Recall macro: 0.942\n",
      "F1 macro: 0.942\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.170\n",
      "loss_avg: 0.170\n",
      "acc_avg: 0.945\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.945\n",
      "Recall macro: 0.945\n",
      "F1 macro: 0.945\n",
      "Validation F1-macro_all: 0.945\n",
      "\n",
      "\n",
      "Epoch [9]:\n",
      "\n",
      "Train:\n",
      "objective: 0.180\n",
      "loss_avg: 0.180\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.165\n",
      "loss_avg: 0.165\n",
      "acc_avg: 0.949\n",
      "\n",
      "objective: 0.166\n",
      "loss_avg: 0.166\n",
      "acc_avg: 0.947\n",
      "\n",
      "objective: 0.175\n",
      "loss_avg: 0.175\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.163\n",
      "loss_avg: 0.163\n",
      "acc_avg: 0.946\n",
      "\n",
      "objective: 0.168\n",
      "loss_avg: 0.168\n",
      "acc_avg: 0.948\n",
      "\n",
      "objective: 0.171\n",
      "loss_avg: 0.171\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.167\n",
      "loss_avg: 0.167\n",
      "acc_avg: 0.947\n",
      "\n",
      "objective: 0.170\n",
      "loss_avg: 0.170\n",
      "acc_avg: 0.948\n",
      "\n",
      "objective: 0.174\n",
      "loss_avg: 0.174\n",
      "acc_avg: 0.947\n",
      "\n",
      "objective: 0.172\n",
      "loss_avg: 0.172\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.179\n",
      "loss_avg: 0.179\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.168\n",
      "loss_avg: 0.168\n",
      "acc_avg: 0.941\n",
      "\n",
      "objective: 0.188\n",
      "loss_avg: 0.188\n",
      "acc_avg: 0.939\n",
      "\n",
      "objective: 0.177\n",
      "loss_avg: 0.177\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.179\n",
      "loss_avg: 0.179\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.173\n",
      "loss_avg: 0.173\n",
      "acc_avg: 0.945\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.945\n",
      "Recall macro: 0.945\n",
      "F1 macro: 0.945\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.173\n",
      "loss_avg: 0.173\n",
      "acc_avg: 0.943\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.943\n",
      "Recall macro: 0.943\n",
      "F1 macro: 0.942\n",
      "Validation F1-macro_all: 0.942\n",
      "\n",
      "\n",
      "Epoch [10]:\n",
      "\n",
      "Train:\n",
      "objective: 0.168\n",
      "loss_avg: 0.168\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.162\n",
      "loss_avg: 0.162\n",
      "acc_avg: 0.950\n",
      "\n",
      "objective: 0.159\n",
      "loss_avg: 0.159\n",
      "acc_avg: 0.950\n",
      "\n",
      "objective: 0.177\n",
      "loss_avg: 0.177\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.170\n",
      "loss_avg: 0.170\n",
      "acc_avg: 0.946\n",
      "\n",
      "objective: 0.171\n",
      "loss_avg: 0.171\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.176\n",
      "loss_avg: 0.176\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.162\n",
      "loss_avg: 0.162\n",
      "acc_avg: 0.950\n",
      "\n",
      "objective: 0.175\n",
      "loss_avg: 0.175\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.170\n",
      "loss_avg: 0.170\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.178\n",
      "loss_avg: 0.178\n",
      "acc_avg: 0.942\n",
      "\n",
      "objective: 0.173\n",
      "loss_avg: 0.173\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.181\n",
      "loss_avg: 0.181\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.161\n",
      "loss_avg: 0.161\n",
      "acc_avg: 0.948\n",
      "\n",
      "objective: 0.170\n",
      "loss_avg: 0.170\n",
      "acc_avg: 0.946\n",
      "\n",
      "objective: 0.182\n",
      "loss_avg: 0.182\n",
      "acc_avg: 0.943\n",
      "\n",
      "objective: 0.175\n",
      "loss_avg: 0.175\n",
      "acc_avg: 0.948\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.946\n",
      "Recall macro: 0.946\n",
      "F1 macro: 0.946\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.167\n",
      "loss_avg: 0.167\n",
      "acc_avg: 0.946\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.946\n",
      "Recall macro: 0.946\n",
      "F1 macro: 0.946\n",
      "Validation F1-macro_all: 0.946\n",
      "\n",
      "\n",
      "Epoch [11]:\n",
      "\n",
      "Train:\n",
      "objective: 0.160\n",
      "loss_avg: 0.160\n",
      "acc_avg: 0.950\n",
      "\n",
      "objective: 0.173\n",
      "loss_avg: 0.173\n",
      "acc_avg: 0.946\n",
      "\n",
      "objective: 0.164\n",
      "loss_avg: 0.164\n",
      "acc_avg: 0.947\n",
      "\n",
      "objective: 0.157\n",
      "loss_avg: 0.157\n",
      "acc_avg: 0.950\n",
      "\n",
      "objective: 0.157\n",
      "loss_avg: 0.157\n",
      "acc_avg: 0.949\n",
      "\n",
      "objective: 0.161\n",
      "loss_avg: 0.161\n",
      "acc_avg: 0.950\n",
      "\n",
      "objective: 0.169\n",
      "loss_avg: 0.169\n",
      "acc_avg: 0.945\n",
      "\n",
      "objective: 0.161\n",
      "loss_avg: 0.161\n",
      "acc_avg: 0.949\n",
      "\n",
      "objective: 0.164\n",
      "loss_avg: 0.164\n",
      "acc_avg: 0.947\n",
      "\n",
      "objective: 0.166\n",
      "loss_avg: 0.166\n",
      "acc_avg: 0.946\n",
      "\n",
      "objective: 0.170\n",
      "loss_avg: 0.170\n",
      "acc_avg: 0.948\n",
      "\n",
      "objective: 0.161\n",
      "loss_avg: 0.161\n",
      "acc_avg: 0.948\n",
      "\n",
      "objective: 0.166\n",
      "loss_avg: 0.166\n",
      "acc_avg: 0.947\n",
      "\n",
      "objective: 0.176\n",
      "loss_avg: 0.176\n",
      "acc_avg: 0.944\n",
      "\n",
      "objective: 0.160\n",
      "loss_avg: 0.160\n",
      "acc_avg: 0.949\n",
      "\n",
      "objective: 0.162\n",
      "loss_avg: 0.162\n",
      "acc_avg: 0.948\n",
      "\n",
      "objective: 0.167\n",
      "loss_avg: 0.167\n",
      "acc_avg: 0.951\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.948\n",
      "Recall macro: 0.948\n",
      "F1 macro: 0.948\n",
      "\n",
      "Validation (OOD/Trans):\n",
      "objective: 0.158\n",
      "loss_avg: 0.158\n",
      "acc_avg: 0.948\n",
      "\n",
      "Epoch eval:\n",
      "Average acc: 0.948\n",
      "Recall macro: 0.948\n",
      "F1 macro: 0.948\n",
      "Validation F1-macro_all: 0.948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(algorithm, wilds_datasets, logger, config, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c533883-1128-49d5-a4d1-497882ccd5e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_y = set()\n",
    "total_m = set()\n",
    "for x,y,m in val_loader_source:\n",
    "    for i in np.unique(y.detach().cpu()):\n",
    "        total_y.add(i)\n",
    "    for i in np.unique(m.detach().cpu()):\n",
    "        total_m.add(i)\n",
    "    if len(x) != len(y):\n",
    "        print('LEN ERROR')\n",
    "        break\n",
    "    #print( np.unique(y), np.unique(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "763dcb1c-68e0-42ed-8d67-fbe465463ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a8ccd4-543b-4f30-9e7b-3a393421b2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cec9fcaa-c9b5-4693-9026-25169b8d1323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <wilds.common.metrics.loss.ElementwiseLoss at 0x1533782cba90>,\n",
       " 'metric': <wilds.common.metrics.all_metrics.Accuracy at 0x1533a09532b0>,\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 3e-05\n",
       "     weight_decay: 0.0\n",
       " ),\n",
       " 'max_grad_norm': None,\n",
       " 'batch_idx': 0,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('model',\n",
       "               ResNet(\n",
       "                 (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "                 (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (relu): ReLU(inplace=True)\n",
       "                 (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "                 (layer1): Sequential(\n",
       "                   (0): Bottleneck(\n",
       "                     (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                     (downsample): Sequential(\n",
       "                       (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): Bottleneck(\n",
       "                     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (2): Bottleneck(\n",
       "                     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (layer2): Sequential(\n",
       "                   (0): Bottleneck(\n",
       "                     (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                     (downsample): Sequential(\n",
       "                       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): Bottleneck(\n",
       "                     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (2): Bottleneck(\n",
       "                     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (3): Bottleneck(\n",
       "                     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (layer3): Sequential(\n",
       "                   (0): Bottleneck(\n",
       "                     (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                     (downsample): Sequential(\n",
       "                       (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                       (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): Bottleneck(\n",
       "                     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (2): Bottleneck(\n",
       "                     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (3): Bottleneck(\n",
       "                     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (4): Bottleneck(\n",
       "                     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (5): Bottleneck(\n",
       "                     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (layer4): Sequential(\n",
       "                   (0): Bottleneck(\n",
       "                     (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                     (downsample): Sequential(\n",
       "                       (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                       (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): Bottleneck(\n",
       "                     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                   (2): Bottleneck(\n",
       "                     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (relu): ReLU(inplace=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                 (fc): Linear(in_features=2048, out_features=13, bias=True)\n",
       "               ))]),\n",
       " 'device': device(type='cuda'),\n",
       " 'out_device': 'cpu',\n",
       " '_has_log': False,\n",
       " 'log_dict': {},\n",
       " 'grouper': <wilds.common.grouper.CombinatorialGrouper at 0x1533782c4fd0>,\n",
       " 'group_prefix': 'group_',\n",
       " 'count_field': 'count',\n",
       " 'group_count_field': 'group_count',\n",
       " 'logged_metrics': [<wilds.common.metrics.loss.ElementwiseLoss at 0x1533782cba90>,\n",
       "  <wilds.common.metrics.all_metrics.Accuracy at 0x1533a09532b0>],\n",
       " 'logged_fields': ['objective'],\n",
       " 'schedulers': [None],\n",
       " 'scheduler_metric_names': [None],\n",
       " 'no_group_logging': True,\n",
       " 'use_unlabeled_y': False,\n",
       " 'breeds': True,\n",
       " 'is_training': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da7f517e-8a30-494a-8d5f-52678c95643b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.eval_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf3f27-4de6-4960-ab64-b7bc16b9e2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
